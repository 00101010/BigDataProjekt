{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from netaddr import IPNetwork, IPAddress\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# c\n",
    "#CITY_BLOCKS_PATH    = ''\n",
    "#CITY_LOCATIONS_PATH = ''\n",
    "#DATA_PATHS          = ''\n",
    "\n",
    "# m Databricks paths\n",
    "CITY_BLOCKS_PATH    = '/FileStore/tables/5ne9pwzz1484570889985/GeoLite2_City_Blocks_IPv4-82d63.csv'\n",
    "CITY_LOCATIONS_PATH = '/FileStore/tables/fcj10i6i1484571614099/GeoLite2_City_Locations_en-6f8fe.csv'\n",
    "DATA_PATHS          = ['/FileStore/tables/xqubiq301484572436491/NetworkTraffic100.csv',\\\n",
    "                       '/FileStore/tables/810vguox1485019521316/100000am.csv',\\\n",
    "                       '/FileStore/tables/810vguox1485019521316/100000pm.csv']\n",
    "\n",
    "# m local paths\n",
    "CITY_BLOCKS_PATH    = 'geolite/GeoLite2-City-Blocks-IPv4.csv'\n",
    "CITY_LOCATIONS_PATH = 'geolite/GeoLite2-City-Locations-en.csv'\n",
    "DATA_PATHS          = ['data/NetworkTraffic100.csv',\\\n",
    "                       'data/100000am.csv',\\\n",
    "                       'data/100000pm.csv',\\\n",
    "                       'data/1000000am.csv',\\\n",
    "                       'data/1000000pm.csv']\n",
    "# m local, from directory\n",
    "DATA_PATHS = [os.path.join('data', file) for file in os.listdir('data') if file.endswith('.csv')]\n",
    "\n",
    "# s\n",
    "#CITY_BLOCKS_PATH    = '/FileStore/tables/jk3parwb1484571151117/GeoLite2_City_Blocks_IPv4-82d63.csv\n",
    "#CITY_LOCATIONS_PATH = '/FileStore/tables/1rn58fwl1484571610700/GeoLite2_City_Locations_en-6f8fe.csv'\n",
    "#DATA_PATHS          = '/FileStore/tables/tjpvf32z1484575566663/100.csv']\n",
    "\n",
    "minColSet            = True # True to restrict the RDD to the required columns\n",
    "persist              = True # True to persist the data RDD. Can lead to warnings 'Not enough memory to cache ...', falls back to file system\n",
    "useBroadcastVariable = True # True to broadcast the reduced network map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isIpInNet(ip, net):\n",
    "  \"\"\"\n",
    "  Checks if a ip is part of a net.\n",
    "  e.g. isIpInNet(\"192.168.0.1\", \"192.168.0.0/24\")\n",
    "  \"\"\"\n",
    "  if IPAddress(ip) in IPNetwork(net):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def buildNetworkCountryMap():\n",
    "  \"\"\"\n",
    "  Builds a map with data from geoIp2 (http://geolite.maxmind.com/download/geoip/database/GeoLite2-City.mmdb.gz)\n",
    "  1st file contains networks and locationkey (much more data available but we consider only this)\n",
    "  2nd file contains locationkey and country name.\n",
    "  method joins this two datarecords\n",
    "  \n",
    "  returns an RDD as [('network', 'country'), ...]\n",
    "  \"\"\"\n",
    "  geoipfile = sc.textFile(CITY_BLOCKS_PATH, 2)\n",
    "  \n",
    "  #(geoname_id, network) / filters the header\n",
    "  geoipdata = geoipfile.map(lambda l:l.split(',')).filter(lambda l:l[0] not in 'network').map(lambda p: (p[1], p[0]))\n",
    "\n",
    "  locationsfile = sc.textFile(CITY_LOCATIONS_PATH, 2)\n",
    "  \n",
    "  #(geoname_id, country_name) / filters the header\n",
    "  locationsdata = locationsfile.map(lambda l:l.split(',')).filter(lambda l:l[0] not in 'geoname_id').map(lambda p: (p[0], p[5]))\n",
    "\n",
    "  #join on geoname_id\n",
    "  joineddata = geoipdata.join(locationsdata)\n",
    "  \n",
    "  #(network, country_name)\n",
    "  networkmap = joineddata.map(lambda d: (d[1][0],d[1][1]))\n",
    "  return networkmap\n",
    "\n",
    "def getCountryByIp(ip):\n",
    "  \"\"\"\n",
    "  Returns the country for the given ip or None if not found\n",
    "  \"\"\"\n",
    "  firstoctet = ip.split(\".\")[0]\n",
    "  \n",
    "  if firstoctet in collectedNetworkMap: \n",
    "    for network in collectedNetworkMap[firstoctet]:\n",
    "      if isIpInNet(ip, network[0]):\n",
    "        return network[1]\n",
    "  return None\n",
    "\n",
    "def getCountryByIpBc(ip):\n",
    "  \"\"\"\n",
    "  Returns the country for the given ip or None if not found; using a broad cast variable.\n",
    "  \"\"\"\n",
    "  firstoctet = ip.split(\".\")[0]\n",
    "  \n",
    "  if firstoctet in reducednetworkmapbc.value: \n",
    "    for network in reducednetworkmapbc.value[firstoctet]:\n",
    "      if isIpInNet(ip, network[0]):\n",
    "        return network[1]\n",
    "  return None\n",
    "\n",
    "def loadData(filePath):\n",
    "  \"\"\" Loads the network traffic data from the specified file.\n",
    "      Args:     path to the file to load.\n",
    "      Returns:  an RDD containing each line (except the header) of the file as a list of values.\n",
    "  \"\"\"\n",
    "  return sc.textFile(filePath).map(lambda l: l.split(\",\")).filter(lambda l:l[0] not in 'ts' and len(l) > 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "networkmap = buildNetworkCountryMap()\n",
    "\n",
    "#We build a map with the first part of the ip, eg. 255 and a list of all the networks to improve performance (max 4.294.967.296 adresse w/ ipv4)\n",
    "reducednetworkmap = networkmap.map(lambda e: (e[0].split(\".\")[0], [e])).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "#print networkmap.count() #2'766'452\n",
    "#print reducednetworkmap.count() #221 => 2'766'452 / 221 = 12'500; 12'500 / 2 = 6'250 mean average (loops) after dictionary access instead of 2'766'452 / 2 = 1'383'226 (calculation supposes a uniform distribution - which is likely not the case)\n",
    "\n",
    "collectedNetworkMap = reducednetworkmap.collectAsMap()\n",
    "if useBroadcastVariable:\n",
    "  reducednetworkmapbc = sc.broadcast(collectedNetworkMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the firewall data\n",
    "data = loadData(DATA_PATHS[0])\n",
    "for path in range(1, len(DATA_PATHS)):\n",
    "  data = data.union(loadData(DATA_PATHS[path]))\n",
    "\n",
    "sourceAddressIndex = 3\n",
    "destinationAddressIndex = 4\n",
    "sourcePortIndex = 5\n",
    "destinationPortIndex = 6\n",
    "protocolIndex = 7\n",
    "inBytesIndex = 12\n",
    "outBytesIndex = 14\n",
    "\n",
    "if minColSet:\n",
    "  data = data.map(lambda r: [r[3], r[4], r[5], r[6], r[7], r[12], r[14]]) # can't use index vars b/c of lazy execution (no closures)\n",
    "  sourceAddressIndex = 0\n",
    "  destinationAddressIndex = 1\n",
    "  sourcePortIndex = 2\n",
    "  destinationPortIndex = 3\n",
    "  protocolIndex = 4\n",
    "  inBytesIndex = 5\n",
    "  outBytesIndex = 6\n",
    "\n",
    "if persist:\n",
    "  data.persist() # lazy excution, (cache is a synonym of persist w/ MEMORY_ONLY storage level)\n",
    "\n",
    "#print data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# requests by country;\n",
    "# PERFORMANCE   2m 26s (8 cores)\n",
    "# minColSet:    2m 22s\n",
    "# minColSet && perist(): (1st call: 2m 32s, subsequent calls: 1m 57s) - few 'Not enough memory to cache ...' warnings\n",
    "# persist():             (1st call: 2m 28s, subsequent calls: 2m 8s) - some 'Not enough memory to cache ...' warnings\n",
    "if useBroadcastVariable:\n",
    "  requestsByCountry = data\\\n",
    "    .map(lambda line: line[sourceAddressIndex]).distinct()\\\n",
    "    .map(lambda sa: (getCountryByIpBc(sa), 1))\\\n",
    "    .filter(lambda r: r[0] is not None)\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .takeOrdered(10, lambda x: -x[1]) # use takeOrdered for small result sets only\n",
    "else:\n",
    "  requestsByCountry = data\\\n",
    "    .map(lambda line: line[sourceAddressIndex]).distinct()\\\n",
    "    .map(lambda sa: (getCountryByIp(sa), 1))\\\n",
    "    .filter(lambda r: r[0] is not None)\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .takeOrdered(10, lambda x: -x[1])\n",
    "  # CPU load about 20% for the first 30-40s, in comparison to 100% from the get go when using broadcast var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize requests by country\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.pie([r[1] for r in requestsByCountry],\\\n",
    "        explode=[0.1 if i == 0 else 0.0 for i, r in enumerate(requestsByCountry)],\\\n",
    "        labels=[r[0] for r in requestsByCountry],\\\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# VPN data definition (no collection)\n",
    "vpnIdentifier = '192.168'\n",
    "vpnData = data.filter(lambda r: r[sourceAddressIndex].startswith(vpnIdentifier) or r[destinationAddressIndex].startswith(vpnIdentifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# requests by protocol\n",
    "print vpnData\\\n",
    "  .map(lambda r: (r[protocolIndex], 1))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# requests by port (53: Namensaufloesung, 3128: proxy)\n",
    "print vpnData\\\n",
    "  .flatMap(lambda r: [(r[sourcePortIndex], 1), (r[destinationPortIndex], 1)])\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .takeOrdered(10, lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# heavy senders in MiB 194.9.121.8: proxy\n",
    "heavySenders = data.map(lambda r: (r[sourceAddressIndex], int(r[outBytesIndex])))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .takeOrdered(20, lambda x: -x[1])\n",
    "\n",
    "print [(x[0], x[1] / 1024.0 / 1024.0) for x in heavySenders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heavy receivers in MiB\n",
    "heavyReceivers = data.map(lambda r: (r[sourceAddressIndex], int(r[inBytesIndex])))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .takeOrdered(20, lambda x: -x[1])\n",
    "\n",
    "print [(x[0], x[1] / 1024.0 / 1024.0) for x in heavyReceivers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# overall data whores in MiB\n",
    "overallDataWhores = data.map(lambda r: (r[sourceAddressIndex], int(r[inBytesIndex]) + int(r[outBytesIndex])))\\\n",
    "  .reduceByKey(lambda a, b: a + b)\\\n",
    "  .takeOrdered(20, lambda x: -x[1])\n",
    "\n",
    "print [(x[0], x[1] / 1024.0 / 1024.0) for x in overallDataWhores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# senders and receivers\n",
    "data\\\n",
    "  .filter(lambda r: int(r[inBytesIndex]) > 0 and int(r[outBytesIndex]) > 0)\\\n",
    "  .map(lambda r: (r[sourceAddressIndex], int(r[inBytesIndex]) + int(r[outBytesIndex])))\\\n",
    "  .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Execution times\n",
    "\n",
    "<!-- would generate an HTML table w/o pre, but makes it hard to read b/c of cell alignment -->\n",
    "<pre>\n",
    "|---------------------------------|------------|-----------|-----------|-----------|\n",
    "| Query                           | all cols   | min cols  | min cols  | min cols  |\n",
    "|                                 | no persist | persist   | persist   | persist   |\n",
    "|                                 |  (8 cores) | (8 cores) | (4 cores) | (2 cores) |\n",
    "|---------------------------------|------------|-----------|-----------|-----------|\n",
    "| Requests by Country             |  3m 36.00s | 3m 43.00s | 4m 12.00s | 5m 18.00s |\n",
    "| Requests by Country (broadcast) |  2m 21.00s | 2m 32.00s | 2m 53.00s | 3m 37.00s |\n",
    "| Requests by Protocol            |     29.00s |     2.90s |     3.28s |     5.41s |\n",
    "| Requests by Port                |     31.70s |     5.01s |     5.90s |     9.08s |\n",
    "| Heavy Senders (in MiB)          |     31.50s |     3.67s |     4.44s |     6.66s |\n",
    "| Heavy Receivers (in MiB)        |     30.65s |     3.61s |     4.71s |     6.93s |\n",
    "| Overall Data Whores (in MiB)    |     31.90s |     4.32s |     5.47s |     8.91s |\n",
    "|---------------------------------|------------|-----------|-----------|-----------|\n",
    "\n",
    "* 1 run, no averages\n",
    "* Requests by Country is the first query and will therefore trigger caching of the RDD ~40s\n",
    "* ~ 800MB Daten\n",
    "* cores 4x2 (HT)\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "name": "geoip",
  "notebookId": 2650015814937534
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
